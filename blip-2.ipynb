{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3108cc33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install seaborn absl-py scikit-image tensorboard huggingface-hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740f84eb-345e-48a1-91a0-6600fce26b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3d26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899aa9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate \n",
    "# !pip install bitsandbytes\n",
    "# !pip uninstall huggingface_hub -y\n",
    "# !pip install huggingface_hub\n",
    "# !pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c66526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\", split=\"train\")\n",
    "test_dataset = load_dataset(\"flaviagiammarino/vqa-rad\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285ca10e-c6fe-4beb-b813-9f90fa6577f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233eb898-020c-4820-9d47-86c63ffa47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130836fc-c15f-42a5-9bdb-0a2cadc405a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c8b581-6da2-464e-9bfd-183bc4117c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = []\n",
    "for d in dataset:\n",
    "    als.append(len(d['answer'].split(' ')))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f906e851-9130-4e81-8f4f-d05937f26fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1321,\n",
       "         2: 229,\n",
       "         3: 104,\n",
       "         4: 54,\n",
       "         5: 28,\n",
       "         6: 15,\n",
       "         7: 15,\n",
       "         8: 12,\n",
       "         9: 6,\n",
       "         14: 4,\n",
       "         13: 3,\n",
       "         17: 1,\n",
       "         10: 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5baa9d5d-60a2-4323-addd-e3a5bd00904d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101d2e31-0470-4bf7-9545-540aef624e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422b28c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afe5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA (v2) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor, image_size=224, convert_mode='RGB', qtype='CLOSED'):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),  # Resize to a fixed size\n",
    "            transforms.ToTensor(),  # Convert to tensor\n",
    "            transforms.ConvertImageDtype(torch.float),  # Ensure float dtype for images\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize (example for RGB)\n",
    "        ])\n",
    "        if convert_mode.upper() == 'RGB':\n",
    "            self.convert = transforms.Lambda(lambda img: img.convert('RGB'))\n",
    "        elif convert_mode.upper() == 'GRAYSCALE':\n",
    "            self.convert = transforms.Lambda(lambda img: img.convert('L'))\n",
    "        else:\n",
    "            raise ValueError(\"convert_mode must be 'RGB' or 'GRAYSCALE'\")\n",
    "        # self.indices = []\n",
    "        # for i, d in enumerate(dataset):\n",
    "        #     if 'CLOSED' == qtype and d['answer'] in ['yes', 'no']:\n",
    "        #         self.indices.append(i)\n",
    "        #     elif 'OPEN' == qtype:\n",
    "        #         self.indices.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.indices)\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idxi):\n",
    "        # idx = self.indices[idxi]\n",
    "        idx = idxi\n",
    "        # get image + text\n",
    "        question = self.dataset[idx]['question']\n",
    "        answer = self.dataset[idx]['answer']\n",
    "        image = self.dataset[idx]['image']\n",
    "        image = self.transform(image)\n",
    "        template = \"Question: {} Answer:\"\n",
    "        prompt = template.format(question)\n",
    "        # print(\"text\", text)\n",
    "        encoding = self.processor(image, text=prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\")#.to(device=\"cuda:0\")\n",
    "\n",
    "        # encoding = self.processor(\n",
    "        # image, prompt=text, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        # ).to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "        # print(\"encoding\",encoding.keys())\n",
    "\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, max_length= 8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():  encoding[k] = v.squeeze()\n",
    "        encoding[\"answer\"] = answer\n",
    "        return encoding\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    processed_batch = {}\n",
    "    \n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    pixel_values_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        input_ids_list.append(item['input_ids'])\n",
    "        attention_mask_list.append(item['attention_mask'])\n",
    "        pixel_values_list.append(item['pixel_values'])  # Assuming pixel_values need no padding\n",
    "        labels_list.append(item['labels'])\n",
    "\n",
    "    processed_batch['input_ids'] = pad_sequence(input_ids_list, batch_first=True, padding_value=0)  # Check the appropriate padding_value for your tokenizer\n",
    "    processed_batch['attention_mask'] = pad_sequence(attention_mask_list, batch_first=True, padding_value=0)\n",
    "    processed_batch['labels'] = pad_sequence(labels_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    processed_batch['pixel_values'] = torch.stack(pixel_values_list)#.to(device)\n",
    "\n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d9a4487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ddd98ceecb4b8d9d7a80b3848cdb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from accelerate import accelerator\n",
    "\n",
    "os.environ['HF_HOME'] = '/data/vep52/nlp/huggingface/hub'\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# processor.tokenizer.padding_side='left'\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    # device_map=\"auto\",\n",
    "    cache_dir = os.environ['HF_HOME']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8dbdcc5-8b8a-4915-8873-146c0c685393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec662ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VQADataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32, collate_fn=collate_fn)\n",
    "test_dataset2 = VQADataset(test_dataset, processor)\n",
    "valid_dataloader = DataLoader(test_dataset2, shuffle=False, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8583a818-e6b5-4822-a17f-497ea8d54ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e35b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,709,632 || all params: 3,753,389,568 || trainable%: 0.23204710947819207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0b000563d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Let's define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # target_modules=[\"q_proj\", \"k_proj\"]\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\",],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e391dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...:   0%|                                                                                                                            | 0/57 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/data/vep52/miniforge3/envs/med_vqa/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [03:14<00:00,  3.41s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:38<00:00,  2.55s/it]\n",
      "/data/vep52/miniforge3/envs/med_vqa/lib/python3.10/site-packages/peft/utils/save_and_load.py:158: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Training loss: 7.913472572962443 - Eval Loss: 4.142926708857218 - LR: 4e-05\n",
      "Saved model to Model/blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [03:14<00:00,  3.42s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:38<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 - Training loss: 3.0476418043437756 - Eval Loss: 2.4314714590708415 - LR: 3.6e-05\n",
      "Saved model to Model/blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [03:13<00:00,  3.40s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:38<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 - Training loss: 2.3112905297363016 - Eval Loss: 2.2333776473999025 - LR: 3.24e-05\n",
      "Saved model to Model/blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [03:07<00:00,  3.29s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:34<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 - Training loss: 2.189639639436153 - Eval Loss: 2.1668533722559613 - LR: 2.9160000000000002e-05\n",
      "Saved model to Model/blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [02:55<00:00,  3.09s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:35<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 - Training loss: 2.061184425103037 - Eval Loss: 2.162467916806539 - LR: 2.6244e-05\n",
      "Saved model to Model/blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [02:55<00:00,  3.08s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:35<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 - Training loss: 1.9873758930909007 - Eval Loss: 2.131565284729004 - LR: 2.3619600000000003e-05\n",
      "Saved model to Model/blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [02:56<00:00,  3.10s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:36<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 - Training loss: 1.9572771348451312 - Eval Loss: 2.152964401245117 - LR: 2.1257640000000004e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [02:53<00:00,  3.04s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:34<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 - Training loss: 1.9362181602862842 - Eval Loss: 2.1726272821426393 - LR: 1.9131876000000003e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [02:55<00:00,  3.08s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:35<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 - Training loss: 1.9441430568695068 - Eval Loss: 2.1414822816848753 - LR: 1.7218688400000002e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [03:16<00:00,  3.45s/it]\n",
      "Validating batch: ...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:35<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 - Training loss: 1.9673885650802077 - Eval Loss: 2.15349014600118 - LR: 1.5496819560000003e-05\n",
      "The finetuning process is complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    # Assuming predictions are logits for classification\n",
    "    preds = torch.argmax(predictions, dim=1)  # Convert logits to class predictions\n",
    "    correct_count = (preds == labels).sum().item()  # Count correct predictions\n",
    "    total_count = labels.size(0)  # Total number of labels\n",
    "    accuracy = correct_count / total_count  # Calculate accuracy\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "\n",
    "num_epochs = 10\n",
    "patience = 5\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float32):\n",
    "            # print(\"input_ids\",input_ids.dtype)\n",
    "            # print(\"pixel_values\", pixel_values.dtype)\n",
    "            # print(\"labels\", labels.dtype)\n",
    "            outputs = model(input_ids=input_ids.to(device),\n",
    "                        pixel_values=pixel_values.to(device, dtype=torch.float32),\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels.to(device))\n",
    "\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float32):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "\n",
    "        # Inside your validation loop, after obtaining `outputs`:\n",
    "        # accuracy = compute_accuracy(outputs.logits, labels)  # Calculate accuracy for the current batch\n",
    "        # total_accuracy += accuracy  # Accumulate accuracy\n",
    "\n",
    "    # After the loop, calculate average accuracy over all batches\n",
    "    # validation_accuracy = total_accuracy / len(valid_dataloader)\n",
    "    # print(f\"Validation Accuracy: {validation_accuracy}\")\n",
    "\n",
    "\n",
    "    tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    scheduler.step()\n",
    "    if eval_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"Model/blip-saved-model\", from_pt=True)\n",
    "        pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "        print(\"Saved model to Model/blip-saved-model\")\n",
    "        min_eval_loss = eval_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"The finetuning process is complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13ebb1-dbdc-4c70-b876-9f7e080daaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc98fc2c-1871-4864-8882-a9bd099ec7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  adapter_config.json\tadapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls Model/blip-saved-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73296ede-513f-4310-87f1-7ba59de15361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                | 0/451 [00:00<?, ?it/s]/data/vep52/miniforge3/envs/med_vqa/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "  1%|█▏                                                                                                                                      | 4/451 [00:02<04:43,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0\n",
      "['yes', '', '', 'right', 'leftright kidney']\n",
      "['yes', 'yes', 'no', 'right', 'not seen here']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "references1 = []\n",
    "hypotheses1 = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "c = 0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for batch in tqdm(test_dataset):\n",
    "    # if not batch['answer'] in ['yes','no']:\n",
    "    #     continue\n",
    "    inputs = processor(images=batch['image'], text=batch['question'], return_tensors=\"pt\").to(\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    generated_ids = model.generate(**inputs,\n",
    "                                   max_length=20,\n",
    "                                   temperature=0.5,\n",
    "                                   num_beams=2,\n",
    "                                   early_stopping=True,\n",
    "                                   min_length=1,\n",
    "                                   repetition_penalty=1.81,\n",
    "                                   )\n",
    "    generated_answers = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    hypotheses1.extend(generated_answers)\n",
    "    references1.append(batch['answer'])  # Each reference wrapped in a list\n",
    "    c += 1\n",
    "    if c == 5:\n",
    "        break\n",
    "\n",
    "# Compute BLEU or any suitable metric\n",
    "bleu_score1 = sacrebleu.corpus_bleu(hypotheses1, references1)\n",
    "print(f\"BLEU Score: {bleu_score1.score}\")\n",
    "print(hypotheses1)\n",
    "print(references1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55252108-5953-41f2-926b-20367810b1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc908bf4-3839-40a4-aee2-dea7df4d07c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2713678-5bfa-450a-bd81-188c5c91176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce/blip2-opt-2.7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f4f52f2fa04034a37e0091eeb40d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from accelerate import accelerator\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "\n",
    "peft_model_id = \"Model/blip-saved-model\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "print(config.base_model_name_or_path)\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "processor.tokenizer.padding_side='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d257b762-98da-4ae0-8231-a8c87c2ce1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38dc0f6e-44b5-459d-8fe1-f2c0e6b8e37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007cad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "# model.eval()\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for batch in tqdm(test_dataset):\n",
    "    inputs = processor(images=batch['image'], text=batch['question'], return_tensors=\"pt\").to(\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    generated_ids = model.generate(**inputs,\n",
    "                                   max_length=75,\n",
    "                                   temperature=0,\n",
    "                                   num_beams=2,\n",
    "                                   early_stopping=True,\n",
    "                                   min_length=1,\n",
    "                                   # seed=42\n",
    "                                   )\n",
    "    generated_answers = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    hypotheses.extend(generated_answers)\n",
    "    references.append(batch['answer'])  # Each reference wrapped in a list\n",
    "\n",
    "# Compute BLEU or any suitable metric\n",
    "bleu_score = sacrebleu.corpus_bleu(hypotheses, references)\n",
    "print(f\"BLEU Score: {bleu_score.score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "092390ac-06db-4016-b003-4f7113926c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nononononononononononononononono“no“no“no“no“no“no“'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1db7a1d-03bd-48e7-bc55-9c7e363f06b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea010684-4c38-49a7-b465-a091d66bc276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                                      | 4/451 [00:02<04:43,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0\n",
      "['yesyesyesyesyesyes', 'yesyesyesyesyesyesyesyesyesyes', 'yesyesyesno', 'right sideright sideright sideright sideright side', ' left,no,no,no,no,no,no,']\n",
      "['yes', 'yes', 'no', 'right', 'not seen here']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "# model.eval()\n",
    "references1 = []\n",
    "hypotheses1 = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "c = 0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for batch in tqdm(test_dataset):\n",
    "    inputs = processor(images=batch['image'], text=batch['question'], return_tensors=\"pt\").to(\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    generated_ids = model.generate(**inputs,\n",
    "                                   max_length=20,\n",
    "                                   temperature=0.5,\n",
    "                                   num_beams=2,\n",
    "                                   early_stopping=True,\n",
    "                                   min_length=1,\n",
    "                                   repetition_penalty=0.5,\n",
    "                                   )\n",
    "    generated_answers = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    hypotheses1.extend(generated_answers)\n",
    "    references1.append(batch['answer'])  # Each reference wrapped in a list\n",
    "    c += 1\n",
    "    if c == 5:\n",
    "        break\n",
    "\n",
    "# Compute BLEU or any suitable metric\n",
    "bleu_score1 = sacrebleu.corpus_bleu(hypotheses1, references1)\n",
    "print(f\"BLEU Score: {bleu_score1.score}\")\n",
    "print(hypotheses1)\n",
    "print(references1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbedc4b-09d7-4f24-99a7-81d9155b1c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6291fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "for batch in tqdm(test_dataset):\n",
    "#     print(batch['answer'])\n",
    "    references.append(batch['answer']) \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e16e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = ['yes', 'yes', 'yesyesyesnononononononononononononononononononononononono', 'right', 'right', 'yes', 'yes', 'right anterior', 'yes', 'yes', 'yes', 'c', 'no-dense,', 'yes', 'yes', 'yes', 'crayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayray', '', 'yes', '', ' the', 'yes', ' plane', 'yes', 'yes', 'crayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayray', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'dellescircircircircircircircircircircircircircircir', 'c', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', '', ' the', 'yes', 'yes', 'yes', 'yes', 'yesyesyesyesyesyesyesyesnonononononononononononononononononononononono', 'yes', 'yes', ' the left lobe of', 'albicalbicbicbicbicbicbicbicbicbicbicb', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yesno', 'yes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'enhancement', 'yesyesnoyesnononononononononononononononononononononononononononononononononono', 'yes', ' of the pathology', 'dp', 'lpbpbbiostalbbiostalbbiostalbbiostalbbiostalbbiostalbbiostalbbiost', 'yes', 'yes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'al', 'right', 'yes', 'yes', '', '', 'right theright theright theright therightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightright', '', 'yes', 'cw', 'theanaralanaralanararararararararararararararararararararar', ' lobe', 'yes', 'yesyesyesyesyesyesnononononononononononononononononononononononononono', '5', 'no', 'yes', '5', 'd', 'yes', 'yes', ' left lobe', 'yes', 'yes', 'right', 'right siderightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightright', '', 'right', 'yes', 'yes', 'yes', '', 'right', 'the', 'ple', 'ple', 'yes', 'yes', 'yes', '', '', 'yes', 'yes', 'yes', 'yes', 'yes', 'cm', 'yes', 'cflflflflflflflflflflflflflflflflflflflflflflflflflflfl', 'the', '5 plane', ' left', 'right', 'yes', '10cm', '10 cm', 'yes', 'bbibibibibibibiyesgendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergendergender', 'yes', 'yes', 'yes', 'theburaluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluralural', 'c', ' the structures of', 'yes', 'ar', 'ar', 'the', 'yes', 'yes', '', ' hemisphere', 'd', 'right parparparparparparparparparparparparparparparparparparparparparparparparparparparparparparparparparpa', '', 'yesnononononononononononononononononononononononononononononononono', '', '', '', 'yes', 'yes', '', '', 'cave', ' the', 'flray', 'yes', 'left', '', 'cprcprcprcprcprcprcprcngngngngngngngngngngngngngngngng', 'therightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrigh', 'dismalment', 'yes', 'yes', 'yes', 'yes', ' the', 'yesnonononononononononononononononononononononononononononononononono', ' hypodensity', 'yes', 'yes', 'gender', 'yes', 'yes', 'yes', 'yes', ' left', 'yes', 'yesnononononononononononononononononononononononononononononono', 'yes', 'yes', '', ' the least dense region of', 'right', 'yes', 'yes', 'darthiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphiphip', 'yes', 'right', 'yes', 'right', '', '', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'cri', 'noyesyesyesyesyesyesnononononononononononononononononononononononono', 'no', 'yesyesyesyesyesyesyesnononononononononononononononononononono', 'yes', 'yes', 'no', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'yes', 'yes', 'b3b3b5b3b5b3b5b3b5b3b5b3b5b3b5b3b5', 'cort', 'dumumflflflflflflflflflflflflflflflflflflflflflflflflflflflflflfl', 'd', 'yes', 'yes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'yes', 'yes', 'yes', 'yes', 'yes', 'dural', ' pathology', 'dyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'yes', '5', 'yes', 'noyesyesyesyesyesnonononononononononononononononononononononononononono', '', 'yes', 'yes', 'yes', 'flortanflortflortflortflortflortflortflortflortflortflortflortflortflort', 'dortortortortortortortortortortortortortortortortortortortortortortortortortortortortortortortortortortort', 'cri', 'yes', 'cirircircircircircircircircircircircircircircircircir', 'yes', 'yes', 'thebldldldldldldldldldldldldldldldldldldldldldldldldldldldldldldldldldld', 'thecccccccccccccccccccccccccccccccccccc', ' pathology of pathology of pathology of pathology of pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology pathology', 'dum', 'yes', '', 'no', 'yes', 'yes', 'yes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'the -', 'plane ', 'c', 'yes', 'yes', 'yes', 'yes', 'yes', 'lungpfngngngngngngngngngngngngngngngngngngngngngngngngngngngngngngng', 'no', 'yesnononononononononononononononononononononononononononononono', 'yes', 'right ventricleright ventric anteriorright ventric anteriorright ventric anteriorright ventric anteriorright ventric anteriorright ventric anteriorright ventric anterio', 'yes', 'yes', 'yes', 'noyesyesyesyesyesnonononononononononononononononononononononononononono', 'yes', 'yes', 'yes', 'crayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayray', 'yes', 'yes', 'right lobe', ' enhancement of the', 'yes', ' the structure of', '', 'yes', 'yesyesyesnonononononononononononononononononononononononono', 'yes', 'yes', 'right', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'no', 'yes', '', 'yes', 'yes', 'yes', 'yes', 'yes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'yes', 'yes', ' left', 'yes', 'yes', 'yes', 'cm', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'd-', 'cprcngngngngngngngngngngngngngngngngngngngngngngngngngngngngngngngngng', 'yes', 'yes', 'cpr', '', 'yes', 'therightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrightrigh', 'yesyesnonononononononononononononononononononononononononononononono', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', '', 'yes', 'yes', 'yes', 'right', 'yes', 'dcscrancrancrancrancrancrancrancrancrancrancrancrancrancrancran', 'dalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal', 'yes', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'yes', 'cflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflfl', 'yes', 'yes', 'yes', 'yes', '', 'right', 'no', 'es', 'yes', 'cm', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', ' left', ' left', 'right', 'yes', 'yes', 'yes', 'yes', '', 'duraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluraluralural', 'no', '10c5c5c5c5c5c5c5c5c5c5c5c5c5c5c5c', 'yes', 'yes', 'bismcirraleschecirraleschecirraleschecirraleschecirralesche', 'yes', 'ray', '', '', 'yes', 'noyesnononononononononononononononononononononononono', 'yes', 'yes', 'yes', '', 'yes', 'mf', 'mfl', 'dwcwcwcwcwcwcwcwcwcwcwcwcwcwcwcwc', 'yes', 'yes', ' enhancement', '', 'no', 'yes', 'ax', '', 'yes', 'yes', 'yes', 'yes', 'dural', 'dflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflfl', 'al', '', 'yes', 'yes', 'thedyesplenthethethethethethethethethethethethethethethethethethethethethethethethethethethethe', 'yes', 'yes', 'lgflbflbflbflbflbflbflbflbflbflbflbflbflbflbflbflbf', 'yes', '5,5d5d5d5d5d5d5d5d5d5d5d5d5d5d5d', 'yesyesyesyesnoyesnoyesnonononononononononononononononononononononononononono', 'mflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflflfl', 'thedyesubdyesubthethethethethethethethethethethethethethethethethethethethethethethethethethe', 'al', 'al', 'yes', 'yes', '', 'yes', ' lobe', 'the', 'yes', 'dalbortbortbortbortbortbortbortbortbortbortbortbortbortbort', 'c', 'd', 'ple', 'right', 'yes', 'yes', 'yes', 'yes', 'yes', 'yesyesyesnonononononononononononononononononononononononononononononono', 'crayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayrayray', 'yes', 'yes', 'yes', 'yes', 'yes', 'yesyesnononononononononononononononononononononononononononono', '5,', 'yes', 'right', 'noyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes', 'right', 'yes', 'yes', 'yes', '10cm', 'yes', 'yes', 'yes', '', '', 'yes', 'yes', 'right anterior', '', 'yes', 'al', 'yes', 'yes', 'left ', 'yes', ' left']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641157cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeating_unit(s):\n",
    "    # Find the first repeating unit in the string\n",
    "    for i in range(1, len(s)):\n",
    "        if s.startswith(s[i:]):\n",
    "            return s[:i]\n",
    "    return s.split(' ')[0]  # Return the whole string if no repetition is found\n",
    "\n",
    "# Example list of strings\n",
    "strings = hypotheses\n",
    "\n",
    "# Extract the first repeating unit from each string\n",
    "extracted_units = [find_repeating_unit(s) for s in strings]\n",
    "\n",
    "# print(extracted_units)\n",
    "\n",
    "\n",
    "extracted_units_2 = []\n",
    "for f in extracted_units:\n",
    "    if f.startswith('yes'):\n",
    "        extracted_units_2.append('yes')\n",
    "    elif f.startswith('no'):\n",
    "        extracted_units_2.append('no')\n",
    "    else:\n",
    "        extracted_units_2.append(f)\n",
    "\n",
    "# extracted_units_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def calculate_similarity_score(s1, s2):\n",
    "    \"\"\"Calculate the similarity score between two strings.\"\"\"\n",
    "    return difflib.SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "def calculate_difflib_accuracy(predicted_texts, actual_texts, threshold=0.8):\n",
    "    \"\"\"Calculate accuracy based on difflib's similarity scores.\"\"\"\n",
    "    correct_count = 0\n",
    "\n",
    "    for predicted, actual in zip(predicted_texts, actual_texts):\n",
    "        score = calculate_similarity_score(predicted, actual)\n",
    "        if score >= threshold:\n",
    "            correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / len(actual_texts)\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "predicted_texts = extracted_units_2\n",
    "actual_texts =  test_dataset[:]['answer']\n",
    "\n",
    "\n",
    "accuracy = calculate_difflib_accuracy(predicted_texts, actual_texts)\n",
    "print(f\"Fuzzy Partial Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a0e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda8e66-acb6-4bab-9c1a-a29455b9128a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4bbb2-4386-4645-a75b-8ddeb0eb488a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922b8d9-ee79-4220-b63d-ea8f19d14715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739a83b-9494-4168-bf74-d61e60af0e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f542c2e-69ce-4368-adb5-59e69e7c2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "\n",
    "TRL_USE_RICH = os.environ.get(\"TRL_USE_RICH\", False)\n",
    "\n",
    "from trl.commands.cli_utils import init_zero_verbose, SftScriptArguments, TrlParser\n",
    "\n",
    "if TRL_USE_RICH:\n",
    "    init_zero_verbose()\n",
    "    FORMAT = \"%(message)s\"\n",
    "\n",
    "    from rich.console import Console\n",
    "    from rich.logging import RichHandler\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.rich import tqdm\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    RichProgressCallback,\n",
    "    SFTTrainer,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    "    get_kbit_device_map,\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# parser = TrlParser((SftScriptArguments, TrainingArguments, ModelConfig))\n",
    "# args, training_args, model_config = parser.parse_args_and_config()\n",
    "training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)\n",
    "# Force use our print callback\n",
    "if TRL_USE_RICH:\n",
    "    training_args.disable_tqdm = True\n",
    "    console = Console()\n",
    "\n",
    "################\n",
    "# Model, Tokenizer & Processor\n",
    "################\n",
    "# LLAVA_CHAT_TEMPLATE = \"\"\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. {% for message in messages %}{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}{% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}{% if add_generation_prompt %}ASSISTANT: {% endif %}\"\"\"\n",
    "\n",
    "torch_dtype = (\n",
    "    model_config.torch_dtype\n",
    "    if model_config.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_config.torch_dtype)\n",
    ")\n",
    "# quantization_config = get_quantization_config(model_config)\n",
    "# model_kwargs = dict(\n",
    "#     revision=model_config.model_revision,\n",
    "#     trust_remote_code=model_config.trust_remote_code,\n",
    "#     attn_implementation=model_config.attn_implementation,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "#     quantization_config=quantization_config,\n",
    "#     cache_dir = '/data/vep52/nlp/huggingface/hub',\n",
    "#     token='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'\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_config.model_name_or_path, use_fast=True)\n",
    "# tokenizer.chat_template = LLAVA_CHAT_TEMPLATE\n",
    "# processor = AutoProcessor.from_pretrained(model_config.model_name_or_path)\n",
    "# processor.tokenizer = tokenizer\n",
    "\n",
    "# model = LlavaForConditionalGeneration.from_pretrained(model_config.model_name_or_path, **model_kwargs)\n",
    "\n",
    "import os\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "os.environ['HF_HOME'] = '/data/vep52/nlp/huggingface/hub'\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"ybelkada/blip2-opt-2.7b-fp16-sharded\",\n",
    "#     device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    cache_dir = os.environ['HF_HOME']\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "init_context = nullcontext() if not TRL_USE_RICH else console.status(\"[bold green]Initializing the SFTTrainer...\")\n",
    "save_context = (\n",
    "    nullcontext()\n",
    "    if not TRL_USE_RICH\n",
    "    else console.status(f\"[bold green]Training completed! Saving the model to {training_args.output_dir}\")\n",
    ")\n",
    "\n",
    "################\n",
    "# Training\n",
    "################\n",
    "with init_context:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",  # need a dummy field\n",
    "        tokenizer=tokenizer,\n",
    "        peft_config=get_peft_config(model_config),\n",
    "        callbacks=[RichProgressCallback] if TRL_USE_RICH else None,\n",
    "        data_collator=data_collator,\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "with save_context:\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "    trainer.push_to_hub()\n",
    "    if Accelerator().is_main_process:\n",
    "        processor.push_to_hub(training_args.hub_model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47d1b1-577b-4151-a2f3-af73b6ac8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --model_name_or_path=\"ybelkada/blip2-opt-2.7b-fp16-sharded\" \\\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/data/vep52/nlp/huggingface/hub'\n",
    "\n",
    "!python /data/vep52/dl/train_blip2.py \\\n",
    "--model_name_or_path=\"llava-hf/llava-1.5-7b-hf\" \\\n",
    "--cache_dir=os.environ['HF_HOME'] \\\n",
    "--load_in_8bit=True\\\n",
    "--report_to=\"wandb\" \\\n",
    "--learning_rate=1.4e-5 \\\n",
    "--per_device_train_batch_size=8 \\\n",
    "--gradient_accumulation_steps=1 \\\n",
    "--output_dir=\"Model/llava-1.5-7b-hf\" \\\n",
    "--logging_steps=5 \\\n",
    "--num_train_epochs=1 \\\n",
    "--push_to_hub \\\n",
    "--gradient_checkpointing \\\n",
    "--remove_unused_columns=False \\\n",
    "--torch_dtype=float16 \\\n",
    "--fp16=True \\\n",
    "# --dataset_name=\"HuggingFaceH4/llava-instruct-mix-vsft\" \\\n",
    "--use_peft=True \\\n",
    "--lora_r=32 \\\n",
    "--lora_alpha=16 \\\n",
    "--lora_target_modules=\"all-linear\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38746c9b-b3e6-4fbc-8f1e-9dec90001ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59948b9e-3867-476f-b0b1-16c381e161c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2415d2-0997-470c-bf90-381da79ffec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72165f62-79bf-405f-afaa-be9842226bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe19908-07f4-4dbe-a957-aab9a4eb5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_vqa_custom import data_collator\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ca852-03f1-4014-bec9-3559c07f55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\", split=\"train\")\n",
    "test_dataset = load_dataset(\"flaviagiammarino/vqa-rad\", split=\"test\")\n",
    "from dataset_vqa_custom import VQADataset, processor\n",
    "train_dataset = VQADataset(dataset, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b769c1-a6f2-480a-92ce-929f50aed781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f126fd1-ac88-48ae-93bc-38224a0f5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b9892-3100-41b8-b94e-96f087c3ccc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
